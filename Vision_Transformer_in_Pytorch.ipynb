{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGnjnZhDIc8JZ+EuLvNvWv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rudraxx/pytorch/blob/main/Vision_Transformer_in_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vision Transformer (ViT)"
      ],
      "metadata": {
        "id": "oycvhD0BmE3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will be implementing the Vision Transformer network from scratch. We will train the network on CIFAR-10 data.\n",
        "\n",
        "Link to the original Paper:\n",
        "\n",
        "\n",
        "1.   [Attention is All you need](https://arxiv.org/pdf/1706.03762)\n",
        "2.   [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\n",
        "](https://arxiv.org/abs/2010.11929)\n",
        "\n",
        "\n",
        "\n",
        "Articles used as reference:\n",
        "\n",
        "\n",
        "1.   [Article on Medium](https://towardsdatascience.com/implementing-vision-transformer-vit-from-scratch-3e192c6155f0)\n",
        "2.   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "jc_kAVHvnZhA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GZtSi0HxmNWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overview of the VIT Architecture\n",
        "![image.png](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q-mBZkDz7TUnVGw1KPwqOA.png)"
      ],
      "metadata": {
        "id": "iN-_bf3qnuFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.datasets import CIFAR10"
      ],
      "metadata": {
        "id": "xbhH_TFQo-K5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stages of the model pipeline**\n",
        "1. Convert images into patches\n",
        "2. Run the patches through linear layer to get patch embeddings. Layer weights are learnt.\n",
        "3.  Get positional embeddings( sin/cos transform)\n",
        "4. Input to the transformer is the sum of patch and positional embeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "fRIjewSboSZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert images into patches\n",
        "class PatchEmbeddings(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.image_size   = config[\"image_size\"] # Size of the incoming images ih xiw\n",
        "    self.patch_size   = config[\"patch_size\"] # What is the size of the patch hxw\n",
        "    self.num_channels = config[\"num_channels\"] # ch\n",
        "    self.hidden_size  = config[\"hidden_size\"] # d\n",
        "\n",
        "    #Calculate the number of patches from the image and patch size\n",
        "    num_patches = self.image_size // self.patch_size\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "U5k_HtCBmNtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ro-M3o0EmNqv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}