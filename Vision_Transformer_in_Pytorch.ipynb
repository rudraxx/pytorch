{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO5WFKZn79lFFRaZrYFR4tm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rudraxx/pytorch/blob/main/Vision_Transformer_in_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vision Transformer (ViT)"
      ],
      "metadata": {
        "id": "oycvhD0BmE3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will be implementing the Vision Transformer network from scratch. We will train the network on CIFAR-10 data.\n",
        "\n",
        "Link to the original Paper:\n",
        "\n",
        "\n",
        "1.   [Attention is All you need](https://arxiv.org/pdf/1706.03762)\n",
        "2.   [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\n",
        "](https://arxiv.org/abs/2010.11929)\n",
        "\n",
        "\n",
        "\n",
        "Articles used as reference:\n",
        "\n",
        "\n",
        "1.   [Article on Medium](https://towardsdatascience.com/implementing-vision-transformer-vit-from-scratch-3e192c6155f0)\n",
        "2.   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "jc_kAVHvnZhA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GZtSi0HxmNWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overview of the VIT Architecture\n",
        "![image.png](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q-mBZkDz7TUnVGw1KPwqOA.png)"
      ],
      "metadata": {
        "id": "iN-_bf3qnuFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.datasets import CIFAR10"
      ],
      "metadata": {
        "id": "xbhH_TFQo-K5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set up some of the hyper parameters\n",
        "\n",
        "#Load the config\n",
        "config ={\"image_size\": 128, \"patch_size\": 16,\n",
        "         \"num_channels\":3, \"hidden_size\": 17,\n",
        "         \"hidden_dropout_prob\": 0.2}\n"
      ],
      "metadata": {
        "id": "wwIMltBduaFZ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stages of the model pipeline**\n",
        "1. Convert images into patches\n",
        "2. Run the patches through linear layer to get patch embeddings. Layer weights are learnt.\n",
        "3. Add the CLS token as the first token for all instances in the batch\n",
        "3.  Get positional embeddings( sin/cos transform)\n",
        "4. Input to the transformer is the sum of patch and positional embeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "fRIjewSboSZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert images into patches\n",
        "class PatchEmbeddings(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.image_size   = config[\"image_size\"] # Size of the incoming images ih xiw\n",
        "    self.patch_size   = config[\"patch_size\"] # What is the size of the patch hxw\n",
        "    self.num_channels = config[\"num_channels\"] # ch\n",
        "    self.hidden_size  = config[\"hidden_size\"] # d_hidden\n",
        "\n",
        "    #Calculate the number of patches from the image and patch size\n",
        "    self.num_patches = (self.image_size // self.patch_size)**2 # p x p\n",
        "\n",
        "    #Create the projection to convert the images into patches\n",
        "    # This layer should take each patch and convert it into a 1D vector of size (1, d_hidden)\n",
        "    self.projection = nn.Conv2d(self.num_channels, self.hidden_size,\n",
        "                                kernel_size=self.patch_size,\n",
        "                                stride=self.patch_size)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    # x is of shape: (B X num_channels X image_size X image_size)\n",
        "    # Required output is of shape: (B X num_patches X d_hidden)\n",
        "\n",
        "    #1) (B X num_channels X image_size X image_size) -> (B X d_hidden X self.num_patches X self.num_patches)\n",
        "    x = self.projection(x)\n",
        "    #2) (B X d_hidden X self.num_patches X self.num_patches) -> (B X d_hidden X self.num_patches*self.num_patches)\n",
        "    x = x.flatten(2)\n",
        "    #3) (B X d_hidden X self.num_patches*self.num_patches) -> (B X self.num_patches*self.num_patches X d_hidden)\n",
        "    x=x.transpose(1,2)\n",
        "\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5k_HtCBmNtP",
        "outputId": "cfde391c-37d2-411e-de63-54835c3ab242"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tensor shape to the patch embedding model: \n",
            " torch.Size([32, 3, 128, 128]). (B X num_channels X image_size X image_size)\n",
            "After the patch embedding:\n",
            " torch.Size([32, 64, 17]). (B X num_patches X d_hidden)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test out the patch embeddings class\n",
        "x = torch.randn(32,config[\"num_channels\"], config[\"image_size\"], config[\"image_size\"])\n",
        "print(f\"Input tensor shape to the patch embedding model: \\n {x.shape}. (B X num_channels X image_size X image_size)\")\n",
        "\n",
        "\n",
        "patch_embedding = PatchEmbeddings(config)\n",
        "\n",
        "x = patch_embedding(x)\n",
        "print(f\"After the patch embedding:\\n {x.shape}. (B X num_patches X d_hidden)\")\n",
        "\n"
      ],
      "metadata": {
        "id": "eccY_CXb1L9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the CLS token to the beginning of each sequence\n",
        "class Embeddings(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.patch_embeddings = PatchEmbeddings(config)\n",
        "\n",
        "    #Create a learnable [CLS] token. This is added before the first patch,\n",
        "    # so should be the same dimension of the other patches\n",
        "    self.cls_token = nn.Parameter(torch.randn(1, config[\"hidden_size\"]))\n",
        "\n",
        "    # Position embedding is added(summed up) with the B X CLS+num_patches X d_hidden,\n",
        "    # so ensure the shape of position_embedding takes this into account\n",
        "    self.position_embeddings = \\\n",
        "      nn.Parameter(torch.randn(1,self.patch_embeddings.num_patches + 1, config[\"hidden_size\"]))\n",
        "\n",
        "    # Drop out layer\n",
        "    self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    batch_size = x.shape[0]\n",
        "    #Get the patch embeddings\n",
        "    x = self.patch_embeddings(x)\n",
        "\n",
        "    #Add the CLS token to every batch item\n",
        "    cls_tokens = self.cls_token.unsqueeze(0).repeat(batch_size, 1, 1)  # Shape: (B, 1, d_hidden)\n",
        "    # Concat the cls token. Note that the size of the hidden_size stays the same. The number of patches/tokens increases by 1.\n",
        "    # because CLS is added as the first token.\n",
        "    x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "    #Add positional embedding\n",
        "    x = x + self.position_embeddings\n",
        "\n",
        "    # Dropout\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "Ro-M3o0EmNqv"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test code to try the math for addition of the cls to the batch\n",
        "cls_x1 = torch.randn(1,10)\n",
        "print(\"cls_x1\", cls_x1.shape)\n",
        "\n",
        "expanded_cls = cls_x1.unsqueeze(0).repeat(32,1,1)\n",
        "print(\"expanded_cls\", expanded_cls.shape)\n",
        "\n",
        "x = torch.randn(32,200, 10)\n",
        "print(\"x\", x.shape)\n",
        "\n",
        "cat_result = torch.cat((expanded_cls, x), dim=1)\n",
        "print(\"cat_result\", cat_result.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "d9EfHutT1Ooo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test out the embeddings\n",
        "embeddings = Embeddings(config)\n",
        "print(embeddings)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UY0amO12yPKH",
        "outputId": "6dd03575-cf1c-4e61-d98a-49df2c2ddbef"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings(\n",
            "  (patch_embeddings): PatchEmbeddings(\n",
            "    (projection): Conv2d(3, 17, kernel_size=(16, 16), stride=(16, 16))\n",
            "  )\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CLS + Position + Patch embeddings have been created. Next step is to create the Transformer Encoder model"
      ],
      "metadata": {
        "id": "4GyU-_0L1gyG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-head Attention Module\n",
        "\n",
        "The Attention module takes sequence of embeddings as input and computes query, key and value vectors for each embedding"
      ],
      "metadata": {
        "id": "I2sIopMB14QI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "  \"\"\"\n",
        "  A single attention head\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, hidden_size, attention_head_size, dropout, bias=True):\n",
        "    super().__init__()\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "    self.attention_head_size = attention_head_size\n",
        "\n",
        "    #Create the query, key and value projection layers\n",
        "\n",
        "    self.query  = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
        "    self.key    = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
        "    self.value  = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
        "\n"
      ],
      "metadata": {
        "id": "vPquqmvm1r6e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}