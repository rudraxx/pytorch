{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rudraxx/pytorch/blob/main/Vision_Transformer_in_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vision Transformer (ViT)"
      ],
      "metadata": {
        "id": "oycvhD0BmE3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will be implementing the Vision Transformer network from scratch. We will train the network on CIFAR-10 data.\n",
        "\n",
        "Link to the original Paper:\n",
        "\n",
        "\n",
        "1.   [Attention is All you need](https://arxiv.org/pdf/1706.03762)\n",
        "2.   [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\n",
        "](https://arxiv.org/abs/2010.11929)\n",
        "\n",
        "\n",
        "\n",
        "Articles used as reference:\n",
        "\n",
        "\n",
        "1.   [Article on Medium](https://towardsdatascience.com/implementing-vision-transformer-vit-from-scratch-3e192c6155f0)\n",
        "2.   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "jc_kAVHvnZhA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GZtSi0HxmNWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overview of the VIT Architecture\n",
        "![image.png](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q-mBZkDz7TUnVGw1KPwqOA.png)"
      ],
      "metadata": {
        "id": "iN-_bf3qnuFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "import math\n"
      ],
      "metadata": {
        "id": "xbhH_TFQo-K5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set up some of the hyper parameters\n",
        "\n",
        "#Load the config\n",
        "config ={\"image_size\": 128, \"patch_size\": 16,\n",
        "         \"num_channels\":3, \"hidden_size\": 16,\n",
        "         \"hidden_dropout_prob\": 0.2,\n",
        "         \"num_attention_heads\":4,\n",
        "         \"qkv_bias\": False,\n",
        "         \"attention_probs_dropout_prob\": 0.2,\n",
        "         \"hidden_dropout_prob\": 0.2,\n",
        "         \"intermediate_size\": 8,\n",
        "         \"num_hidden_layers\": 2}\n"
      ],
      "metadata": {
        "id": "wwIMltBduaFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the device type\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Selected device: {device}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BB1Uohwuh3rV",
        "outputId": "7705a743-200e-4983-aded-4d97726e6202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stages of the model pipeline**\n",
        "1. Convert images into patches\n",
        "2. Run the patches through linear layer to get patch embeddings. Layer weights are learnt.\n",
        "3. Add the CLS token as the first token for all instances in the batch\n",
        "3.  Get positional embeddings( sin/cos transform)\n",
        "4. Input to the transformer is the sum of patch and positional embeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "fRIjewSboSZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert images into patches\n",
        "class PatchEmbeddings(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.image_size   = config[\"image_size\"] # Size of the incoming images ih xiw\n",
        "    self.patch_size   = config[\"patch_size\"] # What is the size of the patch hxw\n",
        "    self.num_channels = config[\"num_channels\"] # ch\n",
        "    self.hidden_size  = config[\"hidden_size\"] # d_hidden\n",
        "\n",
        "    #Calculate the number of patches from the image and patch size\n",
        "    self.num_patches = (self.image_size // self.patch_size)**2 # p x p\n",
        "\n",
        "    #Create the projection to convert the images into patches\n",
        "    # This layer should take each patch and convert it into a 1D vector of size (1, d_hidden)\n",
        "    self.projection = nn.Conv2d(self.num_channels, self.hidden_size,\n",
        "                                kernel_size=self.patch_size,\n",
        "                                stride=self.patch_size)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    # x is of shape: (B X num_channels X image_size X image_size)\n",
        "    # Required output is of shape: (B X num_patches X d_hidden)\n",
        "\n",
        "    #1) (B X num_channels X image_size X image_size) -> (B X d_hidden X self.num_patches X self.num_patches)\n",
        "    x = self.projection(x)\n",
        "    #2) (B X d_hidden X self.num_patches X self.num_patches) -> (B X d_hidden X self.num_patches*self.num_patches)\n",
        "    x = x.flatten(2)\n",
        "    #3) (B X d_hidden X self.num_patches*self.num_patches) -> (B X self.num_patches*self.num_patches X d_hidden)\n",
        "    x=x.transpose(1,2)\n",
        "\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "U5k_HtCBmNtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test out the patch embeddings class\n",
        "x = torch.randn(32,config[\"num_channels\"], config[\"image_size\"], config[\"image_size\"])\n",
        "print(f\"Input tensor shape to the patch embedding model: \\n {x.shape}. (B X num_channels X image_size X image_size)\")\n",
        "\n",
        "\n",
        "patch_embedding = PatchEmbeddings(config)\n",
        "\n",
        "x = patch_embedding(x)\n",
        "print(f\"After the patch embedding:\\n {x.shape}. (B X num_patches X d_hidden)\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eccY_CXb1L9a",
        "outputId": "56116a7a-8e85-4526-8c16-0f40293a63b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tensor shape to the patch embedding model: \n",
            " torch.Size([32, 3, 128, 128]). (B X num_channels X image_size X image_size)\n",
            "After the patch embedding:\n",
            " torch.Size([32, 64, 17]). (B X num_patches X d_hidden)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the CLS token to the beginning of each sequence\n",
        "class Embeddings(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.patch_embeddings = PatchEmbeddings(config)\n",
        "\n",
        "    #Create a learnable [CLS] token. This is added before the first patch,\n",
        "    # so should be the same dimension of the other patches\n",
        "    self.cls_token = nn.Parameter(torch.randn(1, config[\"hidden_size\"]))\n",
        "\n",
        "    # Position embedding is added(summed up) with the B X CLS+num_patches X d_hidden,\n",
        "    # so ensure the shape of position_embedding takes this into account\n",
        "    self.position_embeddings = \\\n",
        "      nn.Parameter(torch.randn(1,self.patch_embeddings.num_patches + 1, config[\"hidden_size\"]))\n",
        "\n",
        "    # Drop out layer\n",
        "    self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    batch_size = x.shape[0]\n",
        "    #Get the patch embeddings\n",
        "    x = self.patch_embeddings(x)\n",
        "\n",
        "    #Add the CLS token to every batch item\n",
        "    cls_tokens = self.cls_token.unsqueeze(0).repeat(batch_size, 1, 1)  # Shape: (B, 1, d_hidden)\n",
        "    # Concat the cls token. Note that the size of the hidden_size stays the same. The number of patches/tokens increases by 1.\n",
        "    # because CLS is added as the first token.\n",
        "    x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "    #Add positional embedding\n",
        "    x = x + self.position_embeddings\n",
        "\n",
        "    # Dropout\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "Ro-M3o0EmNqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test code to try the math for addition of the cls to the batch\n",
        "cls_x1 = torch.randn(1,10)\n",
        "print(\"cls_x1\", cls_x1.shape)\n",
        "\n",
        "expanded_cls = cls_x1.unsqueeze(0).repeat(32,1,1)\n",
        "print(\"expanded_cls\", expanded_cls.shape)\n",
        "\n",
        "x = torch.randn(32,200, 10)\n",
        "print(\"x\", x.shape)\n",
        "\n",
        "cat_result = torch.cat((expanded_cls, x), dim=1)\n",
        "print(\"cat_result\", cat_result.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9EfHutT1Ooo",
        "outputId": "a6c9c85f-23c4-45dd-e851-ccdc79de680f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cls_x1 torch.Size([1, 10])\n",
            "expanded_cls torch.Size([32, 1, 10])\n",
            "x torch.Size([32, 200, 10])\n",
            "cat_result torch.Size([32, 201, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test out the embeddings\n",
        "embeddings = Embeddings(config)\n",
        "print(embeddings)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UY0amO12yPKH",
        "outputId": "5cd1ad6f-9069-4eb5-f355-1b2991747dcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings(\n",
            "  (patch_embeddings): PatchEmbeddings(\n",
            "    (projection): Conv2d(3, 17, kernel_size=(16, 16), stride=(16, 16))\n",
            "  )\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CLS + Position + Patch embeddings have been created. Next step is to create the Transformer Encoder model"
      ],
      "metadata": {
        "id": "4GyU-_0L1gyG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-head Attention Module\n",
        "\n",
        "The Attention module takes sequence of embeddings as input and computes query, key and value vectors for each embedding"
      ],
      "metadata": {
        "id": "I2sIopMB14QI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "  \"\"\"\n",
        "  A single attention head\n",
        "  Used in MultiAttentionHead module\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, hidden_size, attention_head_size, prob_dropout, bias=True):\n",
        "    super().__init__()\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "    self.attention_head_size = attention_head_size\n",
        "\n",
        "    #Create the query, key and value projection layers\n",
        "    self.query  = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
        "    self.key    = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
        "    self.value  = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
        "\n",
        "    self.dropout = nn.Dropout(prob_dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Shape of x: (B x 1+num_patches x hidden_size)\n",
        "    #Project the input into query, key, value\n",
        "    # Since the same x vector is used to generate the Q,K,V values,\n",
        "    # this is called self-attention\n",
        "    query = self.query(x) # (B x 1+num_patches x attention_head_size)\n",
        "    key   = self.key(x)   # (B x 1+num_patches x attention_head_size)\n",
        "    value = self.value(x) # (B x 1+num_patches x attention_head_size)\n",
        "\n",
        "    #Calculate the attention scores\n",
        "    # softmax(Q*K.T/sqrt(head_size)) *V\n",
        "    # print(\"query.shape\", query.shape)\n",
        "    # print(\"key.shape\", key.shape)\n",
        "    # print(\"value.shape\", value.shape)\n",
        "\n",
        "    attention_scores = query @ key.transpose(-1,-2) # Q*K.T\n",
        "    attention_scores = attention_scores/self.attention_head_size**0.5 # Q*K.T/sqrt(d_head)\n",
        "    # print(\"attention_score for 0th item, 0th token pre softmax:\", attention_scores[0,0,:])\n",
        "\n",
        "    #For every token( dim=1), get the softmax across all the other tokens.\n",
        "    # At this stage the dims are B X 1+num_patches X 1+num_patches.\n",
        "    # Take the softmax across the last dim, because each entry in dim=1\n",
        "    # represents the score wrt all the other tokens\n",
        "    attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
        "    # print(\"attention_score for 0th item, 0th token post softmax:\", attention_scores[0,0,:])\n",
        "    # print(\"sum of attention_score for 0th item, 0th token post softmax:\", torch.sum(attention_scores[0,0,:]))\n",
        "\n",
        "    # Dropout\n",
        "    attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "    #Now the scores are of the size: B X 1+num_patches X 1+num_patches,\n",
        "    # and they represent the relative weight for other tokens. Multiply this\n",
        "    # to the value matrix to get the update\n",
        "    attention_output = attention_probs@value\n",
        "    # print(\"attention_scores.shape\", attention_scores.shape)\n",
        "\n",
        "    return (attention_output, attention_probs)\n",
        "\n",
        "\n",
        "# Test\n",
        "hidden_size= config[\"hidden_size\"]#14\n",
        "print(hidden_size)\n",
        "\n",
        "attention_head_size = config[\"hidden_size\"] // config[\"num_attention_heads\"]\n",
        "# attention = AttentionHead(, ,\n",
        "#                           prob_dropout=0.2)\n",
        "\n",
        "attention = AttentionHead(hidden_size, attention_head_size,\n",
        "                          prob_dropout=0.2)\n",
        "\n",
        "x = torch.randn(32, 65, hidden_size)\n",
        "print(f\"hidden_size={hidden_size}\")\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"attention_head_size={attention_head_size}\")\n",
        "(attention_output, attention_probs) = attention(x)\n",
        "\n",
        "print(f\"Output attention_output.shape = {attention_output.shape}, attention_probs: {attention_probs.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPquqmvm1r6e",
        "outputId": "d743b3d9-4c57-4a51-ac4e-d1e50cf8c312"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n",
            "hidden_size=16\n",
            "Input shape: torch.Size([32, 65, 16])\n",
            "attention_head_size=4\n",
            "Output attention_output.shape = torch.Size([32, 65, 4]), attention_probs: torch.Size([32, 65, 65])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config[\"hidden_size\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzRiuJouzzhD",
        "outputId": "e593f3aa-0263-4d04-f9dc-9751ec8c6d32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\"\n",
        "  Multi-haed attention module.\n",
        "  Used in Transformer Encoder module\n",
        "  \"\"\"\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.hidden_size = config[\"hidden_size\"]\n",
        "    self.num_attention_heads = config[\"num_attention_heads\"]\n",
        "    # The attention head size is the hidden size divided by the number of attention heads\n",
        "    self.attention_head_size = self.hidden_size//self.num_attention_heads\n",
        "\n",
        "    #Recalculate the total size of head. If the hidden size is not a integer\n",
        "    # multiple of num_attention_heads, the all_head_size value can be different\n",
        "    # than self.hidden_size\n",
        "    self.all_head_size = self.attention_head_size * self.num_attention_heads\n",
        "\n",
        "    # Decide if bias is used in the Q,K,V projection layers\n",
        "    self.qkv_bias = config[\"qkv_bias\"]\n",
        "\n",
        "    #Create the list of attention heads\n",
        "    self.heads = nn.ModuleList([])\n",
        "\n",
        "    for _ in range(self.num_attention_heads):\n",
        "      head = AttentionHead(self.hidden_size, self.attention_head_size,\n",
        "                           config[\"attention_probs_dropout_prob\"], self.qkv_bias)\n",
        "      self.heads.append(head)\n",
        "\n",
        "    #Create a linear layer to project the attention output back to the hidden size\n",
        "    self.output_projection = nn.Linear(self.all_head_size, self.hidden_size)\n",
        "    self.output_dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
        "\n",
        "  def forward(self, x, output_attentions=False):\n",
        "    print(x.shape)\n",
        "    # Calculate the attention output for each attention head\n",
        "    attention_result = [head(x) for head in self.heads]\n",
        "\n",
        "    #Concat the results along the last dim. Recombine to get back the \"hidden_size\" in last dim\n",
        "    attention_output = torch.cat([attention_outputs for (attention_outputs, _) in attention_result], dim=-1)\n",
        "\n",
        "    #Project the concatenated attention output back to hidden_size\n",
        "    attention_output = self.output_projection(attention_output)\n",
        "    attention_output = self.output_dropout(attention_output)\n",
        "\n",
        "    #Return the attention output and probabilities if needed\n",
        "    if not output_attentions:\n",
        "      return (attention_output, None)\n",
        "    else:\n",
        "      attention_probs = torch.cat([attention_prob for (_, attention_prob) in attention_result], dim=1)\n",
        "      return (attention_output, attention_probs)\n",
        "\n",
        "\n",
        "\n",
        "# Test\n",
        "mha = MultiHeadAttention(config)\n",
        "\n",
        "# # Test\n",
        "# attention = AttentionHead(config[\"hidden_size\"], config[\"attention_head_size\"],\n",
        "#                           prob_dropout=0.2)\n",
        "\n",
        "x = torch.randn(32, 65, config[\"hidden_size\"])\n",
        "print(f\"hidden_size={hidden_size}\")\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"attention_head_size={attention_head_size}\")\n",
        "attention_output, attention_probs = mha(x, output_attentions=True)\n",
        "\n",
        "print(f\"Output attention_output.shape = {attention_output.shape}, attention_probs: {attention_probs.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-etsNGz572CX",
        "outputId": "c0380c75-3179-47c0-fbc9-75e5238568f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hidden_size=16\n",
            "Input shape: torch.Size([32, 65, 16])\n",
            "attention_head_size=4\n",
            "torch.Size([32, 65, 16])\n",
            "Output attention_output.shape = torch.Size([32, 65, 16]), attention_probs: torch.Size([32, 260, 65])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Encoder\n",
        "\n",
        "![Encoder structure](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*iHJ8nTDR1CpOSb-o.png)"
      ],
      "metadata": {
        "id": "aw0onY3h3fnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NewGELUActivation(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see\n",
        "    the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, input):\n",
        "        return (\n",
        "            0.5\n",
        "            * input\n",
        "            * (\n",
        "                1.0\n",
        "                + torch.tanh(\n",
        "                    math.sqrt(2.0 / math.pi)\n",
        "                    * (input + 0.044715 * torch.pow(input, 3.0))\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.dense_1 = nn.Linear(config[\"hidden_size\"], config['intermediate_size'])\n",
        "    self.activation = NewGELUActivation()\n",
        "    self.dense_2 = nn.Linear(config['intermediate_size'], config[\"hidden_size\"])\n",
        "    self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.dense_1(x)\n",
        "    x = self.activation(x)\n",
        "    x = self.dense_2(x)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-rdQzY6d3efM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  '''\n",
        "  A single transformer block\n",
        "  '''\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.attention = MultiHeadAttention(config)\n",
        "\n",
        "    self.layernorm_1 = nn.LayerNorm(config[\"hidden_size\"])\n",
        "    self.mlp = MLP(config)\n",
        "    self.layernorm_2 = nn.LayerNorm(config[\"hidden_size\"])\n",
        "\n",
        "  def forward(self, x, output_attentions=False):\n",
        "    xres_1 = x # Keep track for skip connection\n",
        "    x = self.layernorm_1(xres_1)\n",
        "    # Self attention\n",
        "    (attention_output, attention_probs) = self.attention(x, output_attentions=output_attentions)\n",
        "\n",
        "    #Add the first skip connection\n",
        "    xres_2 = attention_output + xres_1 # Add the skip connection\n",
        "    x = self.layernorm_1(xres_2)\n",
        "\n",
        "    #FF Network\n",
        "    x = self.mlp(x)\n",
        "\n",
        "    #Add the second skip connection\n",
        "    x = x + xres_2\n",
        "\n",
        "    # Return the output and attention probs if asked for:\n",
        "\n",
        "    if output_attentions:\n",
        "      return (x, attention_probs)\n",
        "    else:\n",
        "      return (x, None)\n",
        "\n"
      ],
      "metadata": {
        "id": "Lu2uLfp33edF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The transformer encoder stacks multiple Blocks on top of each other\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.blocks = nn.ModuleList([])\n",
        "    for _ in range(config[\"num_hidden_layers\"]):\n",
        "      self.blocks.append(Block(config))\n",
        "\n",
        "  def forward(self, x, output_attentions=False):\n",
        "\n",
        "    #Calculate the transformer block's output for each block\n",
        "    all_attentions = []\n",
        "    for block in self.blocks:\n",
        "      x, attention_probs = block(x, output_attentions)\n",
        "      if output_attentions:\n",
        "        all_attentions.append(attention_probs)\n",
        "    # Return the encoder's output and the attention probabilities\n",
        "\n",
        "    if output_attentions:\n",
        "      return (x, all_attentions)\n",
        "    else:\n",
        "      return (x, None)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pf2jAqMM3eai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ViT for image classification"
      ],
      "metadata": {
        "id": "R3LFiAYX9vjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTClassification(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.config = config\n",
        "    self.image_size = config[\"image_size\"]\n",
        "    self.hidden_size = config[\"hidden_size\"]\n",
        "    self.num_classes = config[\"num_classes\"]\n",
        "\n",
        "\n",
        "    #Create the embedding model\n",
        "    self.embeddings = Embeddings(config)\n",
        "\n",
        "    #Encoder\n",
        "    self.encoder = Encoder(config)\n",
        "\n",
        "    #Linear layer to project the encoder's output to number of classes\n",
        "    self.mlp_final = nn.Linear(self.hidden_size, self.num_classes) #(B x 1+num_patches x hidden_size)\n",
        "\n",
        "    #Initialize the weights\n",
        "    # self.apply(self._init_weights)\n",
        "\n",
        "  def forward(self, x, output_attentions=False):\n",
        "\n",
        "    # Get the embeddings for the imput\n",
        "    x = self.embeddings(x)\n",
        "\n",
        "    #Encoded values\n",
        "    (encoder_output, all_attentions) = self.encoder(x, output_attentions)\n",
        "\n",
        "    # Calculate the logits, take the [CLS] tokens output as the feature vector for the final classification MLP\n",
        "    logits = self.mlp_final(encoder_output[:,0])\n",
        "\n",
        "    #Return the logits and attention probs\n",
        "    if output_attentions:\n",
        "      return (logits, all_attentions)\n",
        "    else:\n",
        "      return (logits, None)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oJkShN1o3eYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test the full model\n",
        "x = torch.randn(12, 3, 32,32)\n",
        "config = {\n",
        "    \"patch_size\": 4,  # Input image size: 32x32 -> 8x8 patches\n",
        "    \"hidden_size\": 48,\n",
        "    \"num_hidden_layers\": 4,\n",
        "    \"num_attention_heads\": 4,\n",
        "    \"intermediate_size\": 4 * 48, # 4 * hidden_size\n",
        "    \"hidden_dropout_prob\": 0.0,\n",
        "    \"attention_probs_dropout_prob\": 0.0,\n",
        "    \"initializer_range\": 0.02,\n",
        "    \"image_size\": 32,\n",
        "    \"num_classes\": 10, # num_classes of CIFAR10\n",
        "    \"num_channels\": 3,\n",
        "    \"qkv_bias\": True,\n",
        "    \"use_faster_attention\": True,\n",
        "}\n",
        "\n",
        "vit = ViTClassification(config)\n",
        "\n",
        "logits, attention_probs = vit(x, output_attentions=False)\n",
        "\n",
        "print(logits.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3oImFf63eVv",
        "outputId": "3e19d76f-e0b4-4770-9be8-ac51f4d64b0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([12, 65, 48])\n",
            "torch.Size([12, 65, 48])\n",
            "torch.Size([12, 65, 48])\n",
            "torch.Size([12, 65, 48])\n",
            "torch.Size([12, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CIFAR10 dataset"
      ],
      "metadata": {
        "id": "1h5bKfohRrLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "    ])\n",
        "\n",
        "ds_train = datasets.CIFAR10(root=\"./data\", train=True,\n",
        "                            download=True, transform=transform)\n"
      ],
      "metadata": {
        "id": "zd4AXZJz3eTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "id": "Ab-DOfF13eQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GsF1TMnW3eN5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}