{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rudraxx/pytorch/blob/main/Vision_Transformer_in_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vision Transformer (ViT)"
      ],
      "metadata": {
        "id": "oycvhD0BmE3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will be implementing the Vision Transformer network from scratch. We will train the network on CIFAR-10 data.\n",
        "\n",
        "Link to the original Paper:\n",
        "\n",
        "\n",
        "1.   [Attention is All you need](https://arxiv.org/pdf/1706.03762)\n",
        "2.   [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\n",
        "](https://arxiv.org/abs/2010.11929)\n",
        "\n",
        "\n",
        "\n",
        "Articles used as reference:\n",
        "\n",
        "\n",
        "1.   [Article on Medium](https://towardsdatascience.com/implementing-vision-transformer-vit-from-scratch-3e192c6155f0)\n",
        "2.   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "jc_kAVHvnZhA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GZtSi0HxmNWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overview of the VIT Architecture\n",
        "![image.png](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q-mBZkDz7TUnVGw1KPwqOA.png)"
      ],
      "metadata": {
        "id": "iN-_bf3qnuFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "# from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "import math\n"
      ],
      "metadata": {
        "id": "xbhH_TFQo-K5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set up some of the hyper parameters\n",
        "\n",
        "#Load the config\n",
        "config ={\"image_size\": 128, \"patch_size\": 16,\n",
        "         \"num_channels\":3, \"hidden_size\": 16,\n",
        "         \"hidden_dropout_prob\": 0.2,\n",
        "         \"num_attention_heads\":4,\n",
        "         \"qkv_bias\": False,\n",
        "         \"attention_probs_dropout_prob\": 0.2,\n",
        "         \"hidden_dropout_prob\": 0.2,\n",
        "         \"intermediate_size\": 8,\n",
        "         \"num_hidden_layers\": 2}\n"
      ],
      "metadata": {
        "id": "wwIMltBduaFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the device type\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Selected device: {device}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BB1Uohwuh3rV",
        "outputId": "7705a743-200e-4983-aded-4d97726e6202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stages of the model pipeline**\n",
        "1. Convert images into patches\n",
        "2. Run the patches through linear layer to get patch embeddings. Layer weights are learnt.\n",
        "3. Add the CLS token as the first token for all instances in the batch\n",
        "3.  Get positional embeddings( sin/cos transform)\n",
        "4. Input to the transformer is the sum of patch and positional embeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "fRIjewSboSZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert images into patches\n",
        "class PatchEmbeddings(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.image_size   = config[\"image_size\"] # Size of the incoming images ih xiw\n",
        "    self.patch_size   = config[\"patch_size\"] # What is the size of the patch hxw\n",
        "    self.num_channels = config[\"num_channels\"] # ch\n",
        "    self.hidden_size  = config[\"hidden_size\"] # d_hidden\n",
        "\n",
        "    #Calculate the number of patches from the image and patch size\n",
        "    self.num_patches = (self.image_size // self.patch_size)**2 # p x p\n",
        "\n",
        "    #Create the projection to convert the images into patches\n",
        "    # This layer should take each patch and convert it into a 1D vector of size (1, d_hidden)\n",
        "    self.projection = nn.Conv2d(self.num_channels, self.hidden_size,\n",
        "                                kernel_size=self.patch_size,\n",
        "                                stride=self.patch_size)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    # x is of shape: (B X num_channels X image_size X image_size)\n",
        "    # Required output is of shape: (B X num_patches X d_hidden)\n",
        "\n",
        "    #1) (B X num_channels X image_size X image_size) -> (B X d_hidden X self.num_patches X self.num_patches)\n",
        "    x = self.projection(x)\n",
        "    #2) (B X d_hidden X self.num_patches X self.num_patches) -> (B X d_hidden X self.num_patches*self.num_patches)\n",
        "    x = x.flatten(2)\n",
        "    #3) (B X d_hidden X self.num_patches*self.num_patches) -> (B X self.num_patches*self.num_patches X d_hidden)\n",
        "    x=x.transpose(1,2)\n",
        "\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "U5k_HtCBmNtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test out the patch embeddings class\n",
        "x = torch.randn(32,config[\"num_channels\"], config[\"image_size\"], config[\"image_size\"])\n",
        "print(f\"Input tensor shape to the patch embedding model: \\n {x.shape}. (B X num_channels X image_size X image_size)\")\n",
        "\n",
        "\n",
        "patch_embedding = PatchEmbeddings(config)\n",
        "\n",
        "x = patch_embedding(x)\n",
        "print(f\"After the patch embedding:\\n {x.shape}. (B X num_patches X d_hidden)\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eccY_CXb1L9a",
        "outputId": "56116a7a-8e85-4526-8c16-0f40293a63b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tensor shape to the patch embedding model: \n",
            " torch.Size([32, 3, 128, 128]). (B X num_channels X image_size X image_size)\n",
            "After the patch embedding:\n",
            " torch.Size([32, 64, 17]). (B X num_patches X d_hidden)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the CLS token to the beginning of each sequence\n",
        "class Embeddings(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.patch_embeddings = PatchEmbeddings(config)\n",
        "\n",
        "    #Create a learnable [CLS] token. This is added before the first patch,\n",
        "    # so should be the same dimension of the other patches\n",
        "    self.cls_token = nn.Parameter(torch.randn(1, config[\"hidden_size\"]))\n",
        "\n",
        "    # Position embedding is added(summed up) with the B X CLS+num_patches X d_hidden,\n",
        "    # so ensure the shape of position_embedding takes this into account\n",
        "    self.position_embeddings = \\\n",
        "      nn.Parameter(torch.randn(1,self.patch_embeddings.num_patches + 1, config[\"hidden_size\"]))\n",
        "\n",
        "    # Drop out layer\n",
        "    self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    batch_size = x.shape[0]\n",
        "    #Get the patch embeddings\n",
        "    x = self.patch_embeddings(x)\n",
        "\n",
        "    #Add the CLS token to every batch item\n",
        "    cls_tokens = self.cls_token.unsqueeze(0).repeat(batch_size, 1, 1)  # Shape: (B, 1, d_hidden)\n",
        "    # Concat the cls token. Note that the size of the hidden_size stays the same. The number of patches/tokens increases by 1.\n",
        "    # because CLS is added as the first token.\n",
        "    x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "    #Add positional embedding\n",
        "    x = x + self.position_embeddings\n",
        "\n",
        "    # Dropout\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "Ro-M3o0EmNqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test code to try the math for addition of the cls to the batch\n",
        "cls_x1 = torch.randn(1,10)\n",
        "print(\"cls_x1\", cls_x1.shape)\n",
        "\n",
        "expanded_cls = cls_x1.unsqueeze(0).repeat(32,1,1)\n",
        "print(\"expanded_cls\", expanded_cls.shape)\n",
        "\n",
        "x = torch.randn(32,200, 10)\n",
        "print(\"x\", x.shape)\n",
        "\n",
        "cat_result = torch.cat((expanded_cls, x), dim=1)\n",
        "print(\"cat_result\", cat_result.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9EfHutT1Ooo",
        "outputId": "a6c9c85f-23c4-45dd-e851-ccdc79de680f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cls_x1 torch.Size([1, 10])\n",
            "expanded_cls torch.Size([32, 1, 10])\n",
            "x torch.Size([32, 200, 10])\n",
            "cat_result torch.Size([32, 201, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test out the embeddings\n",
        "embeddings = Embeddings(config)\n",
        "print(embeddings)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UY0amO12yPKH",
        "outputId": "5cd1ad6f-9069-4eb5-f355-1b2991747dcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings(\n",
            "  (patch_embeddings): PatchEmbeddings(\n",
            "    (projection): Conv2d(3, 17, kernel_size=(16, 16), stride=(16, 16))\n",
            "  )\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CLS + Position + Patch embeddings have been created. Next step is to create the Transformer Encoder model"
      ],
      "metadata": {
        "id": "4GyU-_0L1gyG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-head Attention Module\n",
        "\n",
        "The Attention module takes sequence of embeddings as input and computes query, key and value vectors for each embedding"
      ],
      "metadata": {
        "id": "I2sIopMB14QI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "  \"\"\"\n",
        "  A single attention head\n",
        "  Used in MultiAttentionHead module\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, hidden_size, attention_head_size, prob_dropout, bias=True):\n",
        "    super().__init__()\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "    self.attention_head_size = attention_head_size\n",
        "\n",
        "    #Create the query, key and value projection layers\n",
        "    self.query  = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
        "    self.key    = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
        "    self.value  = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
        "\n",
        "    self.dropout = nn.Dropout(prob_dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Shape of x: (B x 1+num_patches x hidden_size)\n",
        "    #Project the input into query, key, value\n",
        "    # Since the same x vector is used to generate the Q,K,V values,\n",
        "    # this is called self-attention\n",
        "    query = self.query(x) # (B x 1+num_patches x attention_head_size)\n",
        "    key   = self.key(x)   # (B x 1+num_patches x attention_head_size)\n",
        "    value = self.value(x) # (B x 1+num_patches x attention_head_size)\n",
        "\n",
        "    #Calculate the attention scores\n",
        "    # softmax(Q*K.T/sqrt(head_size)) *V\n",
        "    # print(\"query.shape\", query.shape)\n",
        "    # print(\"key.shape\", key.shape)\n",
        "    # print(\"value.shape\", value.shape)\n",
        "\n",
        "    attention_scores = query @ key.transpose(-1,-2) # Q*K.T\n",
        "    attention_scores = attention_scores/self.attention_head_size**0.5 # Q*K.T/sqrt(d_head)\n",
        "    # print(\"attention_score for 0th item, 0th token pre softmax:\", attention_scores[0,0,:])\n",
        "\n",
        "    #For every token( dim=1), get the softmax across all the other tokens.\n",
        "    # At this stage the dims are B X 1+num_patches X 1+num_patches.\n",
        "    # Take the softmax across the last dim, because each entry in dim=1\n",
        "    # represents the score wrt all the other tokens\n",
        "    attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
        "    # print(\"attention_score for 0th item, 0th token post softmax:\", attention_scores[0,0,:])\n",
        "    # print(\"sum of attention_score for 0th item, 0th token post softmax:\", torch.sum(attention_scores[0,0,:]))\n",
        "\n",
        "    # Dropout\n",
        "    attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "    #Now the scores are of the size: B X 1+num_patches X 1+num_patches,\n",
        "    # and they represent the relative weight for other tokens. Multiply this\n",
        "    # to the value matrix to get the update\n",
        "    attention_output = attention_probs@value\n",
        "    # print(\"attention_scores.shape\", attention_scores.shape)\n",
        "\n",
        "    return (attention_output, attention_probs)\n",
        "\n",
        "\n",
        "# Test\n",
        "hidden_size= config[\"hidden_size\"]#14\n",
        "print(hidden_size)\n",
        "\n",
        "attention_head_size = config[\"hidden_size\"] // config[\"num_attention_heads\"]\n",
        "# attention = AttentionHead(, ,\n",
        "#                           prob_dropout=0.2)\n",
        "\n",
        "attention = AttentionHead(hidden_size, attention_head_size,\n",
        "                          prob_dropout=0.2)\n",
        "\n",
        "x = torch.randn(32, 65, hidden_size)\n",
        "print(f\"hidden_size={hidden_size}\")\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"attention_head_size={attention_head_size}\")\n",
        "(attention_output, attention_probs) = attention(x)\n",
        "\n",
        "print(f\"Output attention_output.shape = {attention_output.shape}, attention_probs: {attention_probs.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPquqmvm1r6e",
        "outputId": "d743b3d9-4c57-4a51-ac4e-d1e50cf8c312"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n",
            "hidden_size=16\n",
            "Input shape: torch.Size([32, 65, 16])\n",
            "attention_head_size=4\n",
            "Output attention_output.shape = torch.Size([32, 65, 4]), attention_probs: torch.Size([32, 65, 65])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config[\"hidden_size\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzRiuJouzzhD",
        "outputId": "e593f3aa-0263-4d04-f9dc-9751ec8c6d32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\"\n",
        "  Multi-haed attention module.\n",
        "  Used in Transformer Encoder module\n",
        "  \"\"\"\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.hidden_size = config[\"hidden_size\"]\n",
        "    self.num_attention_heads = config[\"num_attention_heads\"]\n",
        "    # The attention head size is the hidden size divided by the number of attention heads\n",
        "    self.attention_head_size = self.hidden_size//self.num_attention_heads\n",
        "\n",
        "    #Recalculate the total size of head. If the hidden size is not a integer\n",
        "    # multiple of num_attention_heads, the all_head_size value can be different\n",
        "    # than self.hidden_size\n",
        "    self.all_head_size = self.attention_head_size * self.num_attention_heads\n",
        "\n",
        "    # Decide if bias is used in the Q,K,V projection layers\n",
        "    self.qkv_bias = config[\"qkv_bias\"]\n",
        "\n",
        "    #Create the list of attention heads\n",
        "    self.heads = nn.ModuleList([])\n",
        "\n",
        "    for _ in range(self.num_attention_heads):\n",
        "      head = AttentionHead(self.hidden_size, self.attention_head_size,\n",
        "                           config[\"attention_probs_dropout_prob\"], self.qkv_bias)\n",
        "      self.heads.append(head)\n",
        "\n",
        "    #Create a linear layer to project the attention output back to the hidden size\n",
        "    self.output_projection = nn.Linear(self.all_head_size, self.hidden_size)\n",
        "    self.output_dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
        "\n",
        "  def forward(self, x, output_attentions=False):\n",
        "    print(x.shape)\n",
        "    # Calculate the attention output for each attention head\n",
        "    attention_result = [head(x) for head in self.heads]\n",
        "\n",
        "    #Concat the results along the last dim. Recombine to get back the \"hidden_size\" in last dim\n",
        "    attention_output = torch.cat([attention_outputs for (attention_outputs, _) in attention_result], dim=-1)\n",
        "\n",
        "    #Project the concatenated attention output back to hidden_size\n",
        "    attention_output = self.output_projection(attention_output)\n",
        "    attention_output = self.output_dropout(attention_output)\n",
        "\n",
        "    #Return the attention output and probabilities if needed\n",
        "    if not output_attentions:\n",
        "      return (attention_output, None)\n",
        "    else:\n",
        "      attention_probs = torch.cat([attention_prob for (_, attention_prob) in attention_result], dim=1)\n",
        "      return (attention_output, attention_probs)\n",
        "\n",
        "\n",
        "\n",
        "# Test\n",
        "mha = MultiHeadAttention(config)\n",
        "\n",
        "# # Test\n",
        "# attention = AttentionHead(config[\"hidden_size\"], config[\"attention_head_size\"],\n",
        "#                           prob_dropout=0.2)\n",
        "\n",
        "x = torch.randn(32, 65, config[\"hidden_size\"])\n",
        "print(f\"hidden_size={hidden_size}\")\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"attention_head_size={attention_head_size}\")\n",
        "attention_output, attention_probs = mha(x, output_attentions=True)\n",
        "\n",
        "print(f\"Output attention_output.shape = {attention_output.shape}, attention_probs: {attention_probs.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-etsNGz572CX",
        "outputId": "c0380c75-3179-47c0-fbc9-75e5238568f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hidden_size=16\n",
            "Input shape: torch.Size([32, 65, 16])\n",
            "attention_head_size=4\n",
            "torch.Size([32, 65, 16])\n",
            "Output attention_output.shape = torch.Size([32, 65, 16]), attention_probs: torch.Size([32, 260, 65])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Encoder\n",
        "\n",
        "![Encoder structure](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*iHJ8nTDR1CpOSb-o.png)"
      ],
      "metadata": {
        "id": "aw0onY3h3fnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NewGELUActivation(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see\n",
        "    the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, input):\n",
        "        return (\n",
        "            0.5\n",
        "            * input\n",
        "            * (\n",
        "                1.0\n",
        "                + torch.tanh(\n",
        "                    math.sqrt(2.0 / math.pi)\n",
        "                    * (input + 0.044715 * torch.pow(input, 3.0))\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.dense_1 = nn.Linear(config[\"hidden_size\"], config['intermediate_size'])\n",
        "    self.activation = NewGELUActivation()\n",
        "    self.dense_2 = nn.Linear(config['intermediate_size'], config[\"hidden_size\"])\n",
        "    self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.dense_1(x)\n",
        "    x = self.activation(x)\n",
        "    x = self.dense_2(x)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-rdQzY6d3efM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  '''\n",
        "  A single transformer block\n",
        "  '''\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.attention = MultiHeadAttention(config)\n",
        "\n",
        "    self.layernorm_1 = nn.LayerNorm(config[\"hidden_size\"])\n",
        "    self.mlp = MLP(config)\n",
        "    self.layernorm_2 = nn.LayerNorm(config[\"hidden_size\"])\n",
        "\n",
        "  def forward(self, x, output_attentions=False):\n",
        "    xres_1 = x # Keep track for skip connection\n",
        "    x = self.layernorm_1(xres_1)\n",
        "    # Self attention\n",
        "    (attention_output, attention_probs) = self.attention(x, output_attentions=output_attentions)\n",
        "\n",
        "    #Add the first skip connection\n",
        "    xres_2 = attention_output + xres_1 # Add the skip connection\n",
        "    x = self.layernorm_1(xres_2)\n",
        "\n",
        "    #FF Network\n",
        "    x = self.mlp(x)\n",
        "\n",
        "    #Add the second skip connection\n",
        "    x = x + xres_2\n",
        "\n",
        "    # Return the output and attention probs if asked for:\n",
        "\n",
        "    if output_attentions:\n",
        "      return (x, attention_probs)\n",
        "    else:\n",
        "      return (x, None)\n",
        "\n"
      ],
      "metadata": {
        "id": "Lu2uLfp33edF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The transformer encoder stacks multiple Blocks on top of each other\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.blocks = nn.ModuleList([])\n",
        "    for _ in range(config[\"num_hidden_layers\"]):\n",
        "      self.blocks.append(Block(config))\n",
        "\n",
        "  def forward(self, x, output_attentions=False):\n",
        "\n",
        "    #Calculate the transformer block's output for each block\n",
        "    all_attentions = []\n",
        "    for block in self.blocks:\n",
        "      x, attention_probs = block(x, output_attentions)\n",
        "      if output_attentions:\n",
        "        all_attentions.append(attention_probs)\n",
        "    # Return the encoder's output and the attention probabilities\n",
        "\n",
        "    if output_attentions:\n",
        "      return (x, all_attentions)\n",
        "    else:\n",
        "      return (x, None)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pf2jAqMM3eai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ViT for image classification"
      ],
      "metadata": {
        "id": "R3LFiAYX9vjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTClassification(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.config = config\n",
        "    self.image_size = config[\"image_size\"]\n",
        "    self.hidden_size = config[\"hidden_size\"]\n",
        "    self.num_classes = config[\"num_classes\"]\n",
        "\n",
        "\n",
        "    #Create the embedding model\n",
        "    self.embeddings = Embeddings(config)\n",
        "\n",
        "    #Encoder\n",
        "    self.encoder = Encoder(config)\n",
        "\n",
        "    #Linear layer to project the encoder's output to number of classes\n",
        "    self.mlp_final = nn.Linear(self.hidden_size, self.num_classes) #(B x 1+num_patches x hidden_size)\n",
        "\n",
        "    #Initialize the weights\n",
        "    # self.apply(self._init_weights)\n",
        "\n",
        "  def forward(self, x, output_attentions=False):\n",
        "\n",
        "    # Get the embeddings for the imput\n",
        "    x = self.embeddings(x)\n",
        "\n",
        "    #Encoded values\n",
        "    (encoder_output, all_attentions) = self.encoder(x, output_attentions)\n",
        "\n",
        "    # Calculate the logits, take the [CLS] tokens output as the feature vector for the final classification MLP\n",
        "    logits = self.mlp_final(encoder_output[:,0])\n",
        "\n",
        "    #Return the logits and attention probs\n",
        "    if output_attentions:\n",
        "      return (logits, all_attentions)\n",
        "    else:\n",
        "      return (logits, None)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oJkShN1o3eYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test the full model\n",
        "x = torch.randn(12, 3, 32,32)\n",
        "config = {\n",
        "    \"patch_size\": 4,  # Input image size: 32x32 -> 8x8 patches\n",
        "    \"hidden_size\": 48,\n",
        "    \"num_hidden_layers\": 4,\n",
        "    \"num_attention_heads\": 4,\n",
        "    \"intermediate_size\": 4 * 48, # 4 * hidden_size\n",
        "    \"hidden_dropout_prob\": 0.0,\n",
        "    \"attention_probs_dropout_prob\": 0.0,\n",
        "    \"initializer_range\": 0.02,\n",
        "    \"image_size\": 32,\n",
        "    \"num_classes\": 10, # num_classes of CIFAR10\n",
        "    \"num_channels\": 3,\n",
        "    \"qkv_bias\": True,\n",
        "    \"use_faster_attention\": True,\n",
        "}\n",
        "\n",
        "vit = ViTClassification(config)\n",
        "\n",
        "logits, attention_probs = vit(x, output_attentions=False)\n",
        "\n",
        "print(logits.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3oImFf63eVv",
        "outputId": "3e19d76f-e0b4-4770-9be8-ac51f4d64b0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([12, 65, 48])\n",
            "torch.Size([12, 65, 48])\n",
            "torch.Size([12, 65, 48])\n",
            "torch.Size([12, 65, 48])\n",
            "torch.Size([12, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CIFAR10 dataset"
      ],
      "metadata": {
        "id": "1h5bKfohRrLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Hyperparams\n",
        "BATCH_SIZE=4\n",
        "EPOCHS = 10\n",
        "LR = 1e-3\n",
        "\n",
        "\n",
        "transform = torchvision.transforms.Compose(\n",
        "    [torchvision.transforms.ToTensor(),\n",
        "     torchvision.transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "    ])\n",
        "\n",
        "ds_train = torchvision.datasets.CIFAR10(root=\"./data\", train=True,\n",
        "                            download=True, transform=transform)\n",
        "\n",
        "dl_train = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "ds_test = torchvision.datasets.CIFAR10(root=\"./data\", train=False,\n",
        "                            download=True, transform=transform)\n",
        "\n",
        "dl_test = DataLoader(ds_test, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "classes_dict = {i: curr_class for i,curr_class in enumerate(ds_train.classes)}\n",
        "print(classes_dict)"
      ],
      "metadata": {
        "id": "zd4AXZJz3eTZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b07aa46a-6641-4d7f-e88f-43def3611267"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 48411503.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "{0: 'airplane', 1: 'automobile', 2: 'bird', 3: 'cat', 4: 'deer', 5: 'dog', 6: 'frog', 7: 'horse', 8: 'ship', 9: 'truck'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Show some images\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "images, labels = next(iter(dl_train))\n",
        "print(torch.min(images), torch.max(images), images.shape)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.imshow(torchvision.utils.make_grid(images/2+0.5).permute(1,2,0))\n",
        "print([classes_dict[x] for x in labels.tolist()])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "PfpYLhyrWurI",
        "outputId": "696837fa-4f9a-4c2b-b5a1-aa27f5a69dc6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-1.) tensor(0.9922) torch.Size([4, 3, 32, 32])\n",
            "['deer', 'truck', 'bird', 'bird']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Training\n",
        "\n",
        "loss_criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "for epoch in range(EPOCHS):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(dl_train):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "Ab-DOfF13eQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "GsF1TMnW3eN5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}